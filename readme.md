# ğŸ” Web Scraper Pro

Web Scraper Pro est un **scraper web moderne et sÃ©curisÃ©**, construit avec **Node.js, Express et Cheerio**, offrant une interface utilisateur responsive et une architecture propre basÃ©e sur le modÃ¨le MVC.

---

## âœ¨ FonctionnalitÃ©s principales

* ğŸ¯ **Scraping complet** : titres, paragraphes, images, liens, mÃ©tadonnÃ©es
* ğŸš€ **Performance optimisÃ©e** : systÃ¨me de cache (10 minutes)
* ğŸ” **SÃ©curitÃ© avancÃ©e** : validation dâ€™URL, anti-SSRF, rate limiting
* ğŸ“Š **Statistiques automatiques** : nombre dâ€™Ã©lÃ©ments et nombre de mots
* ğŸ¨ **Interface moderne** : animations, design en dÃ©gradÃ©
* ğŸ“± **Responsive** : compatible mobile, tablette, desktop
* ğŸ“ **Logs avancÃ©s** : Winston pour la journalisation
* âš¡ **Architecture claire** : routes, contrÃ´leurs, services, utils

---

## ğŸ“‹ PrÃ©requis

* Node.js **>= 16.0.0**
* npm **>= 8.0.0**

---

## ğŸš€ Installation

1. Cloner le projet :

```bash
git clone <votre-repo>
cd web-scraper-pro
```

2. Installer les dÃ©pendances :

```bash
npm install
```

3. CrÃ©er le fichier `.env` :

```bash
cp .env.example .env
```

4. CrÃ©er le dossier des logs :

```bash
mkdir logs
```

5. Lancer le serveur :

```bash
npm run dev   # Mode dÃ©veloppement
npm start     # Mode production
```

6. AccÃ©der Ã  lâ€™application :

```
http://localhost:3000
```

---

## ğŸ“ Structure du projet

```
project/
â”œâ”€â”€ server.js
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ scraper.js
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ scraperController.js
â”œâ”€â”€ services/
â”‚   â””â”€â”€ scrapingService.js
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.js
â”‚   â””â”€â”€ logger.js
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js
â”œâ”€â”€ logs/
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration (fichier `.env`)

```env
PORT=3000
NODE_ENV=development
SCRAPE_TIMEOUT=10000
LOG_LEVEL=info
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=20
CACHE_TTL=600
```

---

## ğŸ“¡ API

### **POST /api/scrape**

Scrape une URL et retourne les donnÃ©es extraites.

**Body :**

```json
{
  "url": "https://example.com"
}
```

**RÃ©ponse :**

* Titre
* MÃ©tadonnÃ©es
* Headings
* Paragraphes
* Liens
* Images
* Statistiques
* Source cache ou non

### **GET /api/cache/stats**

Statistiques du cache.

### **DELETE /api/cache/clear**

Nettoyer le cache.

---

## ğŸ›¡ï¸ SÃ©curitÃ© intÃ©grÃ©e

* Validation stricte des URLs
* Protection contre les attaques SSRF
* Blocage des IP locales et privÃ©es
* Rate limiting (20 requÃªtes / 15 min)
* Timeout 10s
* Taille max de rÃ©ponse : 10MB
* Ã‰chappement HTML (anti-XSS)

---

## ğŸ”„ Limitations

* Ne supporte pas les sites nÃ©cessitant JavaScript
* Ne contourne pas les CAPTCHA
* Timeout 10 secondes
* 10MB maximum par rÃ©ponse

---

## ğŸš€ Ã‰volutions prÃ©vues

* [ ] Support de Puppeteer (JS rendering)
* [ ] Export CSV / PDF / JSON
* [ ] Scraping rÃ©cursif (crawler)
* [ ] Authentification
* [ ] SÃ©lecteurs CSS personnalisÃ©s
* [ ] Swagger documentation
* [ ] Docker

---

## ğŸ¤ Contribution

1. Forkez le projet
2. CrÃ©ez une branche
3. Commitez votre contribution
4. Ouvrez une Pull Request

---

## ğŸ‘¨â€ğŸ’» Auteur

ElAyachi Nezar
GitHub : @NezarEa